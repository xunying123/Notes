# 深度学习中的提前退出层

## 介绍

- 模型复杂度在不断增长，但它只对少数复杂的输入具有显著效果，对于大部分的简单的输入反而变得浪费
- 因此对于资源受限的深度学习，我们应当设置合适的退出层，从而使得学习在面对简单的输入时，可以及时的退出，避免资源的浪费。
- 但，曾经的提前退出是基于固定设置的退出判断，设置多了资源浪费，设置少了不能及时退出。
- 现在，提出一种新的退出，它可以被放置在任何一层，根据该层的的预测结果预测出剩余需要层数（即退出层的位置），同时可以调整退出层之前的V和f从而进一步减小资源消耗。

## 背景

- 模型为资源受限的训练平台，训练内容分批次公布，批次之间有一定时间，需要在时间之内处理完计算。
- DVFS动态调整计算运行时的电压与频率
- 运算效率和运行频率是线性的？

## 模型

- 在第一层时开始预测，得到预测值
- 在预测值处判断能否退出，可以，就退出。
- 不可以，就以当前层为值，再次预测。
- 实际预测过程为每一层都算一遍，比较可信度，直到可信度达到一定程度，就退出。

